{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd4eb4e-f164-4b7c-9301-574d962a34b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_images_from_directory(directory, target_shape):\n",
    "    image_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img = cv2.imread(os.path.join(root, filename))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                img = cv2.resize(img, target_shape)  # Resize to the target shape\n",
    "                image_list.append(img)\n",
    "    return image_list\n",
    "\n",
    "target_shape = (256, 256)\n",
    "\n",
    "# Load grayscale and color images from directories with subfolders\n",
    "grayscale_images = load_and_preprocess_images_from_directory(\"bw_output/\", target_shape)\n",
    "color_images = load_and_preprocess_images_from_directory(\"color_input/\", target_shape)\n",
    "print(len(color_images))\n",
    "\n",
    "# Convert grayscale images to single-channel format\n",
    "grayscale_images = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in grayscale_images]\n",
    "\n",
    "# Expand the dimensions of grayscale images to match the model input\n",
    "grayscale_images = [np.expand_dims(img, axis=-1) for img in grayscale_images]\n",
    "\n",
    "# Example: Ensure that the color images are in RGB format\n",
    "for i, img in enumerate(color_images):\n",
    "    if img.shape[2] == 1:\n",
    "        # If color images are in grayscale, convert them to RGB\n",
    "        color_images[i] = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Expand the dimensions of color images to match the model input\n",
    "color_images = [np.expand_dims(img, axis=-1) for img in color_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a21d517-a415-4bba-8c50-95023259edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_colorization_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Encoder\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.UpSampling2D((2, 2)))\n",
    "    \n",
    "    # Output layer with 3 channels (RGB)\n",
    "    model.add(layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b449571c-5a6d-4ae0-b229-c5a822ff10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = create_colorization_model()\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Compile the model with the updated loss function\n",
    "model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980990aa-e623-478f-a893-bfaf33c36642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 68.9%\n",
      "CPU Usage: 29.0%\n",
      "Memory Usage: 69.0%\n",
      "CPU Usage: 79.0%\n",
      "Memory Usage: 69.1%\n",
      "CPU Usage: 81.2%\n",
      "Memory Usage: 68.9%\n",
      "CPU Usage: 79.7%\n",
      "Memory Usage: 67.8%\n",
      "CPU Usage: 84.3%\n",
      "Memory Usage: 67.5%\n",
      "CPU Usage: 84.2%\n",
      "Memory Usage: 67.1%\n",
      "CPU Usage: 83.9%\n",
      "Memory Usage: 66.9%\n",
      "CPU Usage: 77.9%\n",
      "Memory Usage: 68.8%\n",
      "CPU Usage: 85.9%\n",
      "Memory Usage: 69.5%\n",
      "CPU Usage: 89.0%\n",
      "Memory Usage: 69.7%\n",
      "CPU Usage: 86.2%\n",
      "Memory Usage: 68.8%\n",
      "CPU Usage: 84.0%\n",
      "Memory Usage: 68.2%\n",
      "CPU Usage: 81.4%\n",
      "Memory Usage: 69.8%\n",
      "CPU Usage: 88.9%\n",
      "Memory Usage: 71.1%\n",
      "CPU Usage: 90.0%\n",
      "Memory Usage: 70.3%\n",
      "CPU Usage: 86.4%\n",
      "Memory Usage: 69.1%\n",
      "CPU Usage: 86.8%\n",
      "Memory Usage: 68.6%\n",
      "CPU Usage: 88.7%\n",
      "Memory Usage: 68.9%\n",
      "CPU Usage: 87.7%\n",
      "Memory Usage: 68.0%\n",
      "CPU Usage: 85.0%\n",
      "Memory Usage: 68.0%\n",
      "CPU Usage: 83.7%\n",
      "Memory Usage: 68.4%\n",
      "CPU Usage: 81.2%\n",
      "Memory Usage: 68.0%\n",
      "CPU Usage: 78.3%\n",
      "Memory Usage: 68.0%\n",
      "CPU Usage: 79.9%\n",
      "Memory Usage: 68.3%\n",
      "CPU Usage: 80.9%\n",
      "Memory Usage: 68.4%\n",
      "CPU Usage: 81.2%\n",
      "Memory Usage: 67.9%\n",
      "CPU Usage: 83.5%\n",
      "Memory Usage: 67.4%\n",
      "CPU Usage: 77.2%\n",
      "Memory Usage: 68.0%\n",
      "CPU Usage: 89.2%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def check_memory_usage():\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "def check_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# Train the model on grayscale and colored images\n",
    "for epoch in range(1):\n",
    "    for i in range(0, len(grayscale_images), batch_size):\n",
    "        batch_grayscale = np.array(grayscale_images[i:i+batch_size])\n",
    "        batch_color = np.array(color_images[i:i+batch_size])\n",
    "\n",
    "        # Train the model on the batch\n",
    "        model.train_on_batch(batch_grayscale, batch_color)\n",
    "\n",
    "        check_memory_usage()\n",
    "        check_cpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fc8c5-58f0-4784-be8c-14e623341e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Start profiling\n",
    "tf.profiler.experimental.start('logdir')\n",
    "\n",
    "# train the model on grayscale and coloured images\n",
    "model.fit(np.array(grayscale_images), np.array(color_images), epochs=1, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Stop profiling\n",
    "tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640a379-89d9-486c-94da-2bfe2f84f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Save the trained model\n",
    "model.save('models/colorization_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5680c27-7e86-49fc-ab66-4a6d520b04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Load the model and specify the custom loss function\n",
    "trained_model = tf.keras.models.load_model('models/colorization_model.keras', custom_objects={'custom_loss': custom_loss})\n",
    "\n",
    "# Load your grayscale image\n",
    "grayscale_image = cv2.imread('bw_output/Lion/1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Preprocess the grayscale image\n",
    "grayscale_image = cv2.resize(grayscale_image, (256, 256))  # Resize to match the model input size\n",
    "grayscale_image = grayscale_image / 255.0  # Normalize to [0, 1] if needed\n",
    "\n",
    "# Expand the dimensions of the grayscale image to match the model input\n",
    "grayscale_image = np.expand_dims(grayscale_image, axis=-1)\n",
    "\n",
    "# Generate color prediction\n",
    "color_prediction = trained_model.predict(np.array([grayscale_image]))\n",
    "\n",
    "# Post-process the color image\n",
    "color_image = (color_prediction[0] * 255).astype(np.uint8)\n",
    "\n",
    "# Display or save the color image\n",
    "cv2.imwrite('colorized_image.jpg', color_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccc173-dac7-4075-b594-66a0563264b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "train_loss = [19534.6484, 19516.7852, 19516.7852, 19516.7852, 19516.7852, 19516.7852, 19516.7852, 19516.7852, 19516.7852, 19516.7832]\n",
    "val_loss = [14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270, 14517.6270]\n",
    "\n",
    "# Create a range of epochs for x-axis\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23225f48-8ebd-48b4-b2df-1b76e1e94c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The first dimension has size 1, indicating that there is only one example in the batch.\n",
    "# The second and third dimensions have size 256, indicating the height and width of the tensor.\n",
    "# The fourth dimension has size 1, indicating that there is only one channel (e.g., grayscale)\n",
    "input_data = tf.random.normal((1, 256, 256, 1))\n",
    "output_shape = model.compute_output_shape(input_data.shape)\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = model.count_params()\n",
    "\n",
    "# Assuming each parameter requires 2 FLOPs (input and output)\n",
    "flops = 2 * num_params\n",
    "\n",
    "print(f\"Number of Parameters: {num_params}\")\n",
    "print(f\"Estimated FLOPs: {flops}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038356d-0be8-44e5-9934-fc9d17498d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for each layer\n",
    "conv2d_params = 640\n",
    "max_pooling2d_params = 0\n",
    "conv2d_transpose_params = 36928\n",
    "up_sampling2d_params = 0\n",
    "conv2d_1_params = 1731\n",
    "\n",
    "# Input size for each layer\n",
    "input_size = 256 * 256 * 1\n",
    "\n",
    "# Output size for each layer\n",
    "output_size_conv2d = 256 * 256 * 64\n",
    "output_size_max_pooling2d = 128 * 128 * 64\n",
    "output_size_conv2d_transpose = 128 * 128 * 64\n",
    "output_size_up_sampling2d = 256 * 256 * 64\n",
    "output_size_conv2d_1 = 256 * 256 * 3\n",
    "\n",
    "# FLOPs for each layer\n",
    "flops_conv2d = 2 * input_size * output_size_conv2d * conv2d_params\n",
    "flops_max_pooling2d = 2 * output_size_conv2d * output_size_max_pooling2d * max_pooling2d_params\n",
    "flops_conv2d_transpose = 2 * output_size_max_pooling2d * output_size_conv2d_transpose * conv2d_transpose_params\n",
    "flops_up_sampling2d = 2 * output_size_conv2d_transpose * output_size_up_sampling2d * up_sampling2d_params\n",
    "flops_conv2d_1 = 2 * output_size_up_sampling2d * output_size_conv2d_1 * conv2d_1_params\n",
    "\n",
    "total_flops = flops_conv2d + flops_max_pooling2d + flops_conv2d_transpose + flops_up_sampling2d + flops_conv2d_1\n",
    "\n",
    "print(f\"Total number of compuatations: {total_flops}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c40fa9-5f20-4468-9961-8006c41e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the number of parameters at each step\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "model = load_model('models/colorization_model.keras', custom_objects={'custom_loss': custom_loss})\n",
    "\n",
    "model.summary()\n",
    "total_flops = flops_conv2d + flops_max_pooling2d + flops_conv2d_transpose + flops_up_sampling2d + flops_conv2d_1\n",
    "\n",
    "print(f\"Total number of compuatations: {total_flops}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
